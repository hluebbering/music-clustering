---
title: "Data Programming with Python"
author: "Hannah Luebbering"
date: "July 04, 2022"
output: 
  html_document: 
    css: "assets/main2.css"
    toc: yes
    toc_float: yes
knit: (function(inputFile, encoding) {rmarkdown::render(inputFile, encoding = encoding, output_dir = "docs") })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, out.width = "80%")
library(kableExtra)
library(knitr)
library(ggplot2)
library(hrbrthemes)
library(lubridate)
library(GGally)
library(ggrepel)
library(spotifyr)
library(tidyverse)
library(shiny)
library(shinydashboard)
library(magick)

source('scripts/data.R')
```



<script src="assets/min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script src="assets/pymin.js"></script>
<script>hljs.initHighlightingOnLoad();</script>




- Generate a playlist from a song
- Generate a playlist from a playlist
- Edit a playlist


```{r}
mydf2 <- spotifyr::get_my_playlists(limit = 50) %>%
  dplyr::filter(public == TRUE) %>%
  dplyr::select(id, images, name, snapshot_id) %>% 
  dplyr::filter(id != '33SzrwTchxvwouepwsj8KV') %>%
  dplyr::filter(id != '09GJ0uEndlrfFWZXX423Jm') %>%
  dplyr::filter(id != '73zGnfOwzNdzMqymtECwkp') %>%
  dplyr::filter(id != '0cUfOXZrFRuz5xn5J9KwGj') %>%
  dplyr::filter(id != '1nvpVNmzL7Vi1pXcQEiaLx')

ximg <- mydf2$images 
get_images <- list()

for (i in 1:16) {
  myimage <- ximg[i] %>% data.frame() %>% 
    dplyr::filter(height == 640) %>% dplyr::select(url) %>% 
    unlist(use.names = FALSE)
  tiger <- image_read(path = myimage, depth = 16, density = 2000)
  get_images <- append(get_images, tiger)
}

get_images <- image_scale(get_images, "1000x1000")

get_images1 <- image_append(image_scale(get_images[1:4], "x900"))
get_images2 <- image_append(image_scale(get_images[5:8], "x900"))
get_images3 <- image_append(image_scale(get_images[9:12], "x900")) 
get_images4 <- image_append(image_scale(get_images[13:16], "x900")) 
all_images <- c(get_images1, get_images2, get_images3, get_images4)

image_append(image_scale(all_images, "x900"), stack = TRUE)

```


```{r}
x <- image_append(image_scale(all_images, "x280"), stack = TRUE)

image_write(x, path = "my_playlists.png", format = "png",quality = 100,depth = 16,density = 1000)
```









```{r}
mydf <- spotifyr::get_my_playlists(limit = 50) %>%
  dplyr::filter(public == TRUE) %>%
  dplyr::select(id, images, name, snapshot_id) %>% 
  dplyr::filter(id != '33SzrwTchxvwouepwsj8KV')


mydf %>%
  dplyr::select(id, name) %>% 
  kable(row.names = 1:nrow(mydf)) %>%
  kable_styling(font_size = 12, bootstrap_options = c("striped", "condensed"))
```

```{r}
ximg <- mydf$images 
playlist_images <- list()

for (i in 1:20) {
  myimage <- ximg[i] %>% data.frame() %>% 
    dplyr::filter(height == 640) %>% dplyr::select(url) %>% 
    unlist(use.names = FALSE)
  tiger <- image_read(path = myimage, depth = 16, density = 1800)
  playlist_images <- append(playlist_images, tiger)
}

playlist_images <- image_scale(playlist_images, "800x800")

playlist_images1A <- image_append(image_scale(playlist_images[1:10], "x900"))
playlist_images1B <- image_append(image_scale(playlist_images[11:20], "x900"))
all_playlist_images <- c(playlist_images1A, playlist_images1B)



x2 <- image_append(image_scale(all_playlist_images, "x250"), stack = TRUE)
image_write(x2, path = "my_playlists2.png", format = "png",quality = 100,depth = 16,density = 1000)
```



rsconnect::setAccountInfo(name='hannahluebbering',
			  token='4F76484E220B724471AD0F7F930EE60F',
			  secret='y62n0cQV9aM3iHebdXKFxsJ4OBnLYaBKc12kVK/b')










```{r echo=FALSE}
df2 <- dplyr::filter(df, playlist_name == 'but my feet in bottega') 

df3 <- df2 %>% dplyr::select(track_name, artist_name, album_name,popularity, release_date) %>% dplyr::arrange(desc(popularity))

kable(df3 %>% head(12)) %>% kable_styling(bootstrap_options = c('striped'))

```



```{r}
juice_wrld <- spotifyr::get_artist_audio_features(artist = 'Juice Wrld')
juice_wrld <- juice_wrld %>% dplyr::select('track_name','artist_name','artist_id','artists','album_release_date','album_name','danceability','energy','key','loudness','speechiness','acousticness','instrumentalness','liveness','valence','tempo','duration_ms','key_name')

juice_wrld_id <- unique(juice_wrld$artist_id)
juice_wrld_top_tracks <- spotifyr::get_artist_top_tracks(juice_wrld_id)
juice_wrld_top_tracks <- juice_wrld_top_tracks %>%
  dplyr::select('artists', 'name','popularity','album.name')
```





```{r}
plotA <- ggplot(data = juice_wrld) +
  geom_bar(mapping = aes(x=album_name, y = energy),stat = 'identity') +
  geom_point(data=juice_wrld_top_tracks, mapping = aes(x=album.name,color=name, y =as.numeric(sqrt(popularity+popularity^1000)^0.5), cex=popularity))  +
  coord_flip() +
  hrbrthemes::theme_ipsum_rc() +
  theme(legend.position = "none")

plotA
```






------------------------------------------------





## Comparing Multiple Playlists



<span class = "myhighlight">Overview:</span> Given a list of playlists, find all duplicated songs.








### Part 1.


Write a function called `common_artists(artists)` that takes in a list of sets representing musical artist names and returns a float representing the fraction of common artists shared among the sets. The **fraction of common artists** is the number of shared artists divided by the total number of unique artists that appeared across all sets. 


Our assumptions include that the given list `artists` contains at least one set, although the length of the set may or may not be empty. In the case that artists contains at least one empty set, return 0.0.




By definition, a set is a data structure that contains values where order does not matter and there're no duplicates. To find common elements in both set1 and set2, we run `set1 & set2`.



<div class = "termy">



```{python}
def common_artists(artists):
  # Create empty dictionary for unique artists
  artist_counts = dict()
  
  # Loop through list of sets
  for a_set in artists:
    
    # Loop through names in set
    for name in a_set:
      
      # Update current artist's count
      if name in artist_counts:
        artist_counts[name] += 1
      else:
        artist_counts[name] = 1
  
  # Total number of unique artists across all sets
  unique_artists = len(artist_counts)
  shared_artists = 0
  
  # Find artists shared across all sets
  for freq in artist_counts.values():
    if freq == len(artists):
      shared_artists += 1
      
      
  return(float(shared_artists / unique_artists))
```






```{python}
common_artists([
  {"Kanye West", "Post Malone"}, {"Kanye West", "Miley Cyrus", "Post Malone"},
  {"Kanye West", "Justin Bieber"}])
```




</div>



There is one artist (Kanye West) shared across all sets while there are four unique artists (Kanye West, Post Malone, Miley Cyrus, and Justin Bieber). Thus, our fraction of common artists percentage is $1/4 = 0.25$.






















-----------------------------------





### Part 2.


<span class = "myhighlight5">Problem 2</span>


Write a function called `explore_seattle(seattle_map, curr_location)`, where `seattle_map` is a list of tuples. Each tuple has two elements: the first element is the name of the location and the second element is a tuple containing the x and y-coordinates of the given location. `curr_location` is a tuple giving your x and y-coordinates of your location.


You will want to calculate the distance between you and all of the locations and return a list of all the location names. This list should be sorted first by distance between you and the location in ascending order, and any ties in distance are broken by alphabetical order of the location names.







The **euclidean distance** is a simple way to calculate the distance between two data points representing the $x$ and $y$-coordinates of the given location. To calculate the euclidean distance, we use the Pythagorean theorem. If we have two data points $a=\left[a_1, a_2, \ldots , a_n\right]$ and $b=\left[b_1, b_2, \ldots , b_n\right]$, where $n$ is the dimension, we define the euclidean distance between both points as

$$
\begin{align}
D (a,b) &=
\sqrt{(a_1 - b_1)^2 + (a_2-b_2)^2 + \ldots + (a_n-b_n)^2}
\end{align}
$$







By definition, tuples are immutable, which means that we cannot change elements (reassignment, adding, removing). We create a tuple using the command `()`. A tuple has the same operations as lists, such as indexing, slicing, len(), and more.





-----------------------------------



### Part 3.


At the core of your adoption business is an algorithm that uses prospective adopters’ preferences to find the best kitten to take home. There are four qualities of kittens that help to make a good match: `rowdiness`, `hungriness`, `cuddliness`, and `loudness`. You have created a database to track these qualities, which are placed on an integer point scale. 

Optionally, the adopter can specify one of the qualities as a **dealbreaker**: if the kitten’s score for that quality is more than 10 points away from the adopter’s preference, you do not recommend it to the adopter.

Write a function which takes in your kitten database `kittens` (dictionary that has kitten names as keys and dictionary with kitten preferences as values) and adopter preferences dictionary `kitten_preference`, calculates a similarity score for each kitten in the database, and returns the name of the optimal kitten.


Additionally, your function should also take in a parameter called dealbreaker which can either be the name of a quality in the preference list (e.g. "rowdiness") or None. If a dealbreaker is input into the function, you should not consider any kittens which have a quality score that falls more than 10 points ($>10$) away from the adopter preference. If there are no kittens that are considered, return None.




Hint: use the **means squared error algorithm**. Use the following sample kittens and kitten_preference dictionaries to test your answers:




- kittens = {"blair": {"rowdiness": 100, "hungriness": 100, "cuddliness": 0, "loudness": 27}}
- kitten_preference = {"rowdiness": 70, "hungriness": 50, "cuddliness": 100, "loudness": 10}






\ 





We can calculate how similar scores are by using a metric called the **mean squared error** (MSE). Essentially, the MSE is a number that determines closeness of the two lines, where the larger the number is, the more different the two lines are. If the MSE is 0 then two lines are identical.

The MSE is computed as follows. 

- For each point in one dataset, compute the difference between it and the corresponding point in the other dataset and then square the difference.
- Take the average of these squared differences.


Write a function `mean_squared_error(numbers1, numbers2)` that takes two lists of numbers (you can assume the lists have the same length and are not empty) and returns the mean squared error between the lists.







------------------------------------


